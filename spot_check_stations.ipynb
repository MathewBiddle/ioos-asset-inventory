{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ced0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import netCDF4\n",
    "from erddapy import ERDDAP\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec938fd",
   "metadata": {},
   "source": [
    "Configure the ERDDAP to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abbef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = 'aoos'\n",
    "\n",
    "with open('utils/ra_erddaps.json') as f:\n",
    "    urls = json.load(f)\n",
    "\n",
    "server = urls[ra.lower()]\n",
    "\n",
    "print(server)\n",
    "\n",
    "e = ERDDAP(server=server, protocol=\"tabledap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4d2194",
   "metadata": {},
   "source": [
    "Pull in the processed inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb82725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# df_cruise = df[df['Station Description'] == 'Chesapeake Bay WQ Cruise Data ']\n",
    "#\n",
    "#df_aws = df[df['Station ID'] == 'Humboldt']\n",
    "# #url = 'http://tds.glos.us/thredds/dodsC/buoy_agg_standard/OMOECC_E1/OMOECC_E1.ncml'\n",
    "# #url = 'http://tds.glos.us/thredds/dodsC/buoy_agg_standard/45186/45186.ncml'\n",
    "# #url = 'http://tds.glos.us/thredds/dodsC/buoy_agg_standard/bgsusd2/bgsusd2.ncml'\n",
    "# url = 'http://oos.soest.hawaii.edu/thredds/dodsC/hioos/nss/ns12agg'\n",
    "# print(url)\n",
    "# ds = xr.open_dataset(url)\n",
    "# #ds = netCDF4.Dataset(url,'r')\n",
    "# title = ds.title\n",
    "# start_time = np.datetime_as_string(ds.time.min().values, unit='D')\n",
    "# end_time = np.datetime_as_string(ds.time.max().values, unit='D')\n",
    "# print('Duration: %s - %s' % (np.datetime_as_string(ds.time.min().values, unit='D'),\n",
    "#                              np.datetime_as_string(ds.time.max().values, unit='D'))\n",
    "#       )\n",
    "\n",
    "# Try glos web server\n",
    "# url = 'https://glbuoys.glos.us/tools/export?ftype=csv&data_type=buoy&units=eng&locs=OMOECC_E1&params=Water_Temperature_at_Surface|dissolved_oxygen_saturation|water_conductivity|ysi_turbidity&tperiod=custom&date_start=2020-01-01&date_end=2020-12-31&avg_ivld=none'\n",
    "# df = erddapy.ERDDAP.to_pandas(url)\n",
    "\n",
    "## using ERDDAP to look for stations\n",
    "# check out http://data.glos.us/erddap/tabledap/allDatasets.htmlTable?datasetID%2Ctitle%2CminTime%2CmaxTime&maxTime%3E=2020-01-01&maxTime%3C=2020-12-31&orderBy(%22maxTime%22)\n",
    "# that lists out all the GLOS stations with the maximum time of observations within the year 2020 (on their ERDDAP).\n",
    "\n",
    "df = pd.read_excel('2021/data/processed/%s.xlsx' % ra.upper(), dtype=str)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749db1e0",
   "metadata": {},
   "source": [
    "Search the ERDDAP server for a single station of interest - more of a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d76e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Station ID'].loc[df['Station ID'].isna()] = df.loc[df['Station ID'].isna(),'Dataset ID']\n",
    "\n",
    "# df['Station ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wf = df[df['Station ID'] == 'ioos:station:TCOON:076']\n",
    "\n",
    "# for SCCOOS need to search for 'Station Long Name' as 'Station ID' is NA for most entries\n",
    "search_for = df_wf['Station ID'].to_string()\n",
    "\n",
    "search_url = e.get_search_url(search_for=search_for, response=\"csv\")\n",
    "\n",
    "try:\n",
    "    resp = pd.read_csv(search_url)\n",
    "    print(resp['Station ID'])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60869f5a",
   "metadata": {},
   "source": [
    "Rip through all the datasets in the inventory and see if they exist on the RA erddap. Process checks the following order:\n",
    "\n",
    "1. `Station ID`, then \n",
    "2. `Station Long Name`, then\n",
    "3. `WMO ID or NWS/CMAN ID`\n",
    "\n",
    "We first check if the value is `nan`. If so, we punt as that will return all datasets from the ERDDAP service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found = []\n",
    "found = dict()\n",
    "\n",
    "exclude = ' -gliders'\n",
    "\n",
    "search_order = ['Station Long Name', 'WMO ID or NWS/CMAN ID', 'Station ID']\n",
    "\n",
    "for search1 in df[search_order[0]].astype(str):\n",
    "    \n",
    "    if search1 == \"nan\":\n",
    "        print(\"punting with station long name = nan\\n\")\n",
    "        not_found.append(search1)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        print('searching \\\"%s\\\" = \\\"%s\\\"' % (search_order[0], search1))\n",
    "        search_url = e.get_search_url(search_for=search1+exclude, response=\"csv\")\n",
    "        resp = pd.read_csv(search_url)\n",
    "        \n",
    "        print(\"Found \\\"%s\\\": %s\" % (search1,resp['Dataset ID'].values.tolist()))\n",
    "        \n",
    "        found[search1] = {'Dataset ID': resp['Dataset ID'].values.tolist()}\n",
    "        \n",
    "    except:\n",
    "        \n",
    "        print('no dataset matching \\\"%s\\\"' % search1)\n",
    "        search2 = df.loc[df[search_order[0]] == search1, search_order[1]].values.tolist()[0]\n",
    "        \n",
    "        if search2 == \"nan\":\n",
    "                print(\"punting with %s = nan\\n\" % search_order[1])\n",
    "                not_found.append(search1)\n",
    "                continue\n",
    "                \n",
    "        try:\n",
    "            \n",
    "            print('searching \\\"%s\\\" = \\\"%s\\\"' % (search_order[1], search2))\n",
    "            search_url = e.get_search_url(search_for=search2+exclude, response=\"csv\")\n",
    "            resp = pd.read_csv(search_url)\n",
    "            print(\"Found \\\"%s\\\": %s\" % (search2,resp['Dataset ID'].values.tolist()))\n",
    "            \n",
    "            found[search1] = {'Dataset ID': resp['Dataset ID'].values.tolist()}\n",
    "            \n",
    "        except:\n",
    "\n",
    "            search3 = str(df.loc[df[search_order[0]] == search1, search_order[2]].values.tolist()[0])\n",
    "            \n",
    "            if search3 == \"nan\":\n",
    "                print(\"punting with \\\"%s\\\" = nan\\n\" % search_order[2])\n",
    "                not_found.append(search1)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print('searching \\\"%s\\\" = \\\"%s\\\"' % (search_order[2], search3))\n",
    "                search_url = e.get_search_url(search_for=search3+exclude, response=\"csv\")\n",
    "                resp = pd.read_csv(search_url)\n",
    "                print(\"Found \\\"%s\\\": %s\" % (search3,resp['Dataset ID'].values.tolist()))\n",
    "                \n",
    "                found[search1] = {'Dataset ID': resp['Dataset ID'].values.tolist()}\n",
    "                \n",
    "            except:\n",
    "                print(\"Couldn't find:\", [search1, search2, search3])\n",
    "                not_found.append(search1)\n",
    "        \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d839f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "046acb11",
   "metadata": {},
   "source": [
    "Print the stations that we couldn't find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1930efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456c76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffe63f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[search_order[0]].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[search_order[0]].isin(not_found)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa5716a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#found.pop('HTLPWES')\n",
    "found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630fd36a",
   "metadata": {},
   "source": [
    "Check if the dataset has coverage for CY 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b399859",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_datasets = []\n",
    "\n",
    "for key in found:\n",
    "    \n",
    "    dataset_id = found[key]['Dataset ID']\n",
    "    \n",
    "    if len(dataset_id)>1:\n",
    "        \n",
    "        for dataset in dataset_id:\n",
    "            \n",
    "            e.dataset_id = dataset\n",
    "    \n",
    "            e.constraints = {\n",
    "                 \"time>=\": \"2021-01-01\"\n",
    "            }\n",
    "        \n",
    "            e.variables = ['time']\n",
    "    \n",
    "            try:\n",
    "                df_data = e.to_pandas(parse_dates=True)\n",
    "                valid_datasets.extend([key])\n",
    "                \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "                #print(\"No data for 2021 for %s: dataset ID %s\" % (key,e.dataset_id))\n",
    "\n",
    "    else:\n",
    "        \n",
    "        e.dataset_id = dataset_id[0]\n",
    "    \n",
    "        e.constraints = {\n",
    "                         \"time>=\": \"2021-01-01\"\n",
    "                        }\n",
    "        \n",
    "        e.variables = ['time']\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            df_data = e.to_pandas(parse_dates=True)\n",
    "\n",
    "            valid_datasets.extend([key])\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "            #print(\"No data for 2021 for %s: dataset ID %s\" % (key,e.dataset_id))\n",
    "\n",
    "            \n",
    "#for valid in sorted(set(valid_datasets)):\n",
    "    \n",
    "#     # drop qc vars\n",
    "#     cols = [c for c in df_data.columns if 'qc' not in c]\n",
    "#     cols = [c for c in cols if 'QARTOD' not in c]\n",
    "\n",
    "#     df_data = df_data[cols]\n",
    "\n",
    "#     # set index for plotting\n",
    "#     df_data = df_data.set_index(df_data['time (UTC)'])\n",
    "\n",
    "#     # plot\n",
    "#     #df_data.plot(subplots=True, title=e.dataset_id,figsize=(20,20))\n",
    "\n",
    "#     start_time = df_data['time (UTC)'].min()\n",
    "#     end_time = df_data['time (UTC)'].max()\n",
    "\n",
    "#     print('Dataset %s' % e.dataset_id)\n",
    "#     print('Duration: %s - %s' % (start_time, end_time))\n",
    "\n",
    "non_2021_datasets = found.keys() - sorted(set(valid_datasets))\n",
    "\n",
    "print('Datasets without coverage for 2021:\\n%s' % non_2021_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d9b37",
   "metadata": {},
   "source": [
    "Show me the inventory metadata about the invalid stations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_stations = df.loc[(df[search_order[0]].isin(non_2021_datasets)) | df[search_order[0]].isin(not_found)]\n",
    "\n",
    "df['erddap_not_avail'] = df[search_order[0]].isin(bad_stations[search_order[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43449d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['erddap_not_avail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41685065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#df = df.loc[df['']]\n",
    "\n",
    "df.rename(columns=\n",
    "{'Variable Names + water column depth of measurement in meters [CF_name (# m, # m) or CF_name (mult) or CF_name (# depths)].':\n",
    "     'Variable Names',\n",
    " 'Station Deployment (mm/yyyy, yyyy, < 5 yr, > 5 yr)': 'Station Deployment',\n",
    " 'Longitude (dec deg)': 'Longitude',\n",
    " 'Latitude (dec deg)': 'Latitude'},\n",
    "              inplace=True)\n",
    "\n",
    "df['Station Deployment'] = \\\n",
    "    df['Station Deployment'].astype(str)\n",
    "\n",
    "\n",
    "gdf = geopandas.GeoDataFrame(\n",
    "    df, geometry=geopandas.points_from_xy(df['Longitude'], df['Latitude']))\n",
    "\n",
    "world = geopandas.read_file(geopandas.datasets.get_path('naturalearth_lowres'))\n",
    "\n",
    "ax = world.plot(\n",
    "    color='white', \n",
    "    edgecolor='grey', \n",
    "    figsize=(25,20),\n",
    "    )\n",
    "\n",
    "gdf.plot( ax=ax,\n",
    "         column='erddap_not_avail', \n",
    "         categorical=True, \n",
    "         cmap = 'Set1_r',\n",
    "         markersize=10, \n",
    "         legend=True,\n",
    "         k=2,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "minx, miny, maxx, maxy = gdf.total_bounds\n",
    "ax.set_xlim(minx, maxx)\n",
    "ax.set_ylim(miny, maxy)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf691b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_final = gdf.set_crs(epsg=4326)\n",
    "\n",
    "gdf_final.explore('erddap_not_avail',cmap = 'Set1_r',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf0eb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_final.loc[gdf_final['geometry'].is_empty == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66491b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51710c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# for url in gdf_final.loc[gdf_final['erddap_not_avail'],'Station Description'].str.replace('url: ',''):\n",
    "#     print(url)\n",
    "#     try:\n",
    "#         urllib.request.urlopen(url).getcode()\n",
    "#     except:\n",
    "#         print(\"URL broken: %s\" % url)\n",
    "#         print(gdf_final.loc[gdf_final['Station Description'].str.contains(url,regex=False),'Station ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d9bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b449a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_final['Station Description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7652d138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
